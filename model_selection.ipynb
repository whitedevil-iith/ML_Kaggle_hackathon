{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "train_path = 'train.csv'\n",
    "test_path = 'test.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "train_data = pd.read_csv(train_path)\n",
    "test_data = pd.read_csv(test_path)\n",
    "\n",
    "print(f\"{train_data.columns}, Length = {len(train_data.columns)}\")\n",
    "data = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestModel:\n",
    "    def __init__(self, model=None, train_data_fname=\"'train.csv'\", params_list=None, n_fold_cross_validation=10):\n",
    "        self.model = model  # The model to be tuned\n",
    "        self.originalTrain = pd.read_csv(train_data_fname)\n",
    "        self.originalTest = None\n",
    "        self.train_data = self.pre_process(self.originalTrain, \"train\")  # Training data\n",
    "        # self.test_data = self.pre_process(test_data, \"test\")  # Testing data\n",
    "        self.test_data = None\n",
    "        self.params_list = params_list  # Hyperparameters to tune\n",
    "        self.best_model = None  # To store the best model after tuning\n",
    "        self.best_params = None  # To store the best parameters\n",
    "        self.n_fold_cross_validation = n_fold_cross_validation  # Number of folds for cross-validation\n",
    "        self.columns_to_keep = []  # Columns selected after pre-processing\n",
    "        self.feature_importance = {}  # Store feature importance (MI)\n",
    "\n",
    "    def map_categories_to_numeric(self, data, column_name=\"Target\"):\n",
    "        # Define the category mapping\n",
    "        category_mapping = {'low': 0, 'medium': 1, 'high': 2}\n",
    "        \n",
    "        # Map the column to numeric values\n",
    "        data[column_name] = data[column_name].map(category_mapping)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def replace_missing_values(self, data):\n",
    "        # Iterate over each column and replace missing values\n",
    "        for column in data.columns:\n",
    "            # Check if the column is of type float\n",
    "            if data[column].dtype == 'float64' or data[column].dtype == 'float32':\n",
    "                # Convert floats that represent integers to int\n",
    "                if (data[column] % 1 == 0).all():  # Check if all values in the column are effectively integers\n",
    "                    data[column] = data[column].astype('int64')  # Convert to integer type\n",
    "\n",
    "                # Replace missing values in float columns with the mean\n",
    "                mean_value = data[column].mean()\n",
    "                data[column] = data[column].fillna(mean_value)  # Fixing the warning by avoiding inplace=True\n",
    "\n",
    "            # Check if the column is of type int (or was converted from float to int)\n",
    "            elif data[column].dtype == 'int64' or data[column].dtype == 'int32':\n",
    "                # Replace missing values in integer columns with the mode\n",
    "                mode_value = data[column].mode()[0]\n",
    "                data[column] = data[column].fillna(mode_value)  # Fixing the warning by avoiding inplace=True\n",
    "\n",
    "        return data\n",
    "\n",
    "    def pre_process(self, data, dataType=\"train\"):\n",
    "        if dataType == \"train\":\n",
    "            data = self.map_categories_to_numeric(data)\n",
    "            \n",
    "            # Drop columns with more than a certain threshold of missing values\n",
    "            threshold = 0.8  # 80% missing values\n",
    "            threshold_count = int((1 - threshold) * len(data))\n",
    "            data = data.dropna(axis=1, thresh=threshold_count)\n",
    "\n",
    "            # Replace missing values in columns\n",
    "            data = self.replace_missing_values(data)\n",
    "            \n",
    "            # Remove irrelevant columns based on feature-target relationship\n",
    "            data = self.remove_irrelevant_columns(data)\n",
    "\n",
    "            # Save the columns to keep for consistency with the test set\n",
    "            self.columns_to_keep = list(data.columns)  # Save as a list to maintain order\n",
    "            print(\"Columns to keep:\", self.columns_to_keep)\n",
    "\n",
    "        elif dataType == \"test\":\n",
    "            common_columns = [col for col in self.columns_to_keep if col in data.columns]\n",
    "            columns_to_remove = set(self.originalTest.columns - common_columns)\n",
    "            data = data.drop(columns=columns_to_remove)\n",
    "            data = data.reindex(columns=self.columns_to_keep)\n",
    "            data = self.replace_missing_values(data)\n",
    "            \n",
    "        return data\n",
    "\n",
    "\n",
    "    def remove_irrelevant_columns(self, data):\n",
    "        # Calculate Mutual Information for classification (feature importance)\n",
    "        mutual_info = mutual_info_classif(data.drop('Target', axis=1), data['Target'])\n",
    "        feature_importance = dict(zip(data.drop('Target', axis=1).columns, mutual_info))\n",
    "        \n",
    "        # Threshold for mutual information to decide which columns are irrelevant\n",
    "        irrelevant_columns = [column for column, importance in feature_importance.items() if importance < 0.015]  # You can adjust this threshold\n",
    "        \n",
    "        print(f\"Removing columns with low mutual information: {irrelevant_columns}\")\n",
    "        data = data.drop(columns=irrelevant_columns)\n",
    "\n",
    "        # Pearson and Spearman correlations to remove correlated features\n",
    "        correlation_matrix = data.corr()\n",
    "        for column in data.columns:\n",
    "            try:\n",
    "                # Skip columns that are constant (no variance)\n",
    "                if data[column].nunique() == 1:\n",
    "                    print(f\"Skipping {column} due to constant values.\")\n",
    "                    continue\n",
    "                \n",
    "                # Pearson Correlation with Target\n",
    "                pearson_corr, _ = pearsonr(data[column], data['Target'])\n",
    "                if abs(pearson_corr) < 0.015:  # A low correlation threshold\n",
    "                    print(f\"Removing {column} due to low Pearson correlation with Target: {pearson_corr}\")\n",
    "                    data = data.drop(columns=[column])\n",
    "\n",
    "                # Spearman Correlation with Target\n",
    "                spearman_corr, _ = spearmanr(data[column], data['Target'])\n",
    "                if abs(spearman_corr) < 0.015:  # A low correlation threshold\n",
    "                    print(f\"Removing {column} due to low Spearman correlation with Target: {spearman_corr}\")\n",
    "                    data = data.drop(columns=[column])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating correlation for {column}: {e}\")\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def best_model_outputer(self):\n",
    "        # Define the custom scoring function (F1 score)\n",
    "        f1_scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "        # Perform Grid Search with Stratified K-Fold and F1 score as the metric\n",
    "        grid_search = GridSearchCV(\n",
    "            self.model, \n",
    "            self.params_list, \n",
    "            cv=self.n_fold_cross_validation, \n",
    "            scoring=f1_scorer, \n",
    "            n_jobs=-2, \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Use the entire training data for hyperparameter tuning\n",
    "        grid_search.fit(self.train_data.drop('Target', axis=1), self.train_data['Target'])\n",
    "        \n",
    "        # Store the best model and best parameters\n",
    "        self.best_model = grid_search.best_estimator_\n",
    "        self.best_params = grid_search.best_params_\n",
    "\n",
    "        print(f\"Best parameters: {self.best_params}\")\n",
    "        print(f\"Best F1 score from Grid Search: {grid_search.best_score_}\")\n",
    "\n",
    "        # Perform cross-validation with the best model\n",
    "        kf = StratifiedKFold(n_splits=self.n_fold_cross_validation, shuffle=True, random_state=42)\n",
    "        f1_scores_macro = []\n",
    "\n",
    "        # Cross-validation loop to calculate the macro F1 score for each fold\n",
    "        for train_index, test_index in kf.split(self.train_data.drop('Target', axis=1), self.train_data['Target']):\n",
    "            X_train, X_test = self.train_data.drop('Target', axis=1).iloc[train_index], self.train_data.drop('Target', axis=1).iloc[test_index]\n",
    "            y_train, y_test = self.train_data['Target'].iloc[train_index], self.train_data['Target'].iloc[test_index]\n",
    "            \n",
    "            # Train the model on the training set\n",
    "            self.best_model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict on the test set\n",
    "            y_pred = self.best_model.predict(X_test)\n",
    "            \n",
    "            # Calculate the macro F1 score for this fold\n",
    "            f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "            f1_scores_macro.append(f1_macro)\n",
    "        \n",
    "        # Calculate the mean and standard deviation of the macro F1 scores\n",
    "        mean_f1_macro = np.mean(f1_scores_macro)\n",
    "        std_f1_macro = np.std(f1_scores_macro)\n",
    "\n",
    "        print(f\"\\n\\nCross-validation results:\")\n",
    "        print(f\"\\tMacro F1 Scores: {f1_scores_macro}\")\n",
    "        print(f\"\\tMean Macro F1 Score: {mean_f1_macro:.4f}\")\n",
    "        print(f\"\\tStandard Deviation of Macro F1 Score: {std_f1_macro:.4f}\")\n",
    "\n",
    "        return self.best_model, self.best_params\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        self.best_model.predict(test_data)\n",
    "        reverse_mapping = {0: 'low', 1: 'medium', 2: 'high'}\n",
    "        return pd.Series(data).map(reverse_mapping)\n",
    "    def make_predictions(self, test_fname, predictions_fname):\n",
    "        test_data = pd.read_csv(test_fname)\n",
    "        self.originalTest = test_data\n",
    "        self.test_data = self.pre_process(data=test_data, dataType=\"test\")\n",
    "        predictions = self.predict(self.test_data)\n",
    "\n",
    "        def reverse_map_numeric_to_categories(data):\n",
    "            reverse_mapping = {0: 'low', 1: 'medium', 2: 'high'}\n",
    "            \n",
    "            # Convert the NumPy array to a Pandas Series and apply the reverse mapping\n",
    "            return pd.Series(data).map(reverse_mapping)\n",
    "\n",
    "        # Step 3: Apply the function to the predictions\n",
    "        reversed_predictions = reverse_map_numeric_to_categories(predictions)\n",
    "\n",
    "        # Step 4: Add UID column from `copy_test` DataFrame to the `reversed_predictions`\n",
    "        reversed_predictions_df = pd.DataFrame({\n",
    "            'UID': self.originalTest['UID'],\n",
    "            'Target': reversed_predictions  # The reversed prediction values\n",
    "        })\n",
    "        reversed_predictions_df.to_csv(predictions_fname, index=False)\n",
    "        return reversed_predictions_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage Example\n",
    "# Assuming train_data and test_data are pandas DataFrames, and model is an estimator like XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Example params list (you can adjust this for your model)\n",
    "params_list = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "params_list = {\n",
    "    'learning_rate': [0.5],\n",
    "    'n_estimators': [2],\n",
    "    'max_depth': [3]\n",
    "}\n",
    "\n",
    "\n",
    "# Assuming `train_data` and `test_data` are already defined\n",
    "model = XGBClassifier(random_state=42)\n",
    "\n",
    "# Create an instance of bestModel\n",
    "best_model_instance = BestModel(model, \"train.csv\", params_list)\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model, best_params = best_model_instance.best_model_outputer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best Model of xGBoost with params : {best_params}\")\n",
    "best_model_instance.make_predictions('test.csv','submission.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
