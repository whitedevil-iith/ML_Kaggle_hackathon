{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['UID', 'AgriculturalPostalZone', 'AgricultureZoningCode',\n",
      "       'CropFieldConfiguration', 'CropSpeciesVariety', 'CultivatedAndWildArea',\n",
      "       'CultivatedAreaSqft1', 'DistrictId', 'FarmClassification',\n",
      "       'FarmEquipmentArea', 'FarmShedAreaSqft', 'FarmVehicleCount',\n",
      "       'FarmingCommunityId', 'FarmingUnitCount', 'FieldConstructionType',\n",
      "       'FieldEstablishedYear', 'FieldShadeCover', 'FieldSizeSqft',\n",
      "       'FieldZoneLevel', 'HarvestProcessingType', 'HarvestStorageSqft',\n",
      "       'HasGreenHouse', 'HasPestControl', 'LandUsageType', 'Latitude',\n",
      "       'Longitude', 'MainIrrigationSystemCount', 'NationalRegionCode',\n",
      "       'NaturalLakePresence', 'NumberGreenHouses', 'NumberOfFarmingZones',\n",
      "       'OtherZoningCode', 'PartialIrrigationSystemCount',\n",
      "       'PerimeterGuardPlantsArea', 'PrimaryCropAreaSqft',\n",
      "       'PrimaryCropAreaSqft2', 'RawLocationId', 'ReservoirType',\n",
      "       'ReservoirWithFilter', 'SoilFertilityType', 'StorageAndFacilityCount',\n",
      "       'TaxAgrarianValue', 'TaxLandValue', 'TaxOverdueStatus',\n",
      "       'TaxOverdueYear', 'TotalAreaSqft', 'TotalCultivatedAreaSqft',\n",
      "       'TotalReservoirSize', 'TotalTaxAssessed', 'TotalValue', 'TownId',\n",
      "       'TypeOfIrrigationSystem', 'UndergroundStorageSqft', 'ValuationYear',\n",
      "       'WaterAccessPoints', 'WaterAccessPointsCalc', 'WaterReservoirCount',\n",
      "       'Target'],\n",
      "      dtype='object'), Length = 58\n"
     ]
    }
   ],
   "source": [
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "train_path = '/home/whitedevil/Desktop/Education/FoML/Kaggle_Hackathon/ML_Kaggle_hackathon/train.csv'\n",
    "test_path = '/home/whitedevil/Desktop/Education/FoML/Kaggle_Hackathon/ML_Kaggle_hackathon/test.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "train_data = pd.read_csv(train_path)\n",
    "test_data = pd.read_csv(test_path)\n",
    "\n",
    "print(f\"{train_data.columns}, Length = {len(train_data.columns)}\")\n",
    "data = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestModel:\n",
    "    def __init__(self, model=None, train_data_fname=\"'train.csv'\", params_list=None, n_fold_cross_validation=1):\n",
    "        self.model = model  # The model to be tuned\n",
    "        self.originalTrain = pd.read_csv(train_data_fname)\n",
    "        self.originalTest = None\n",
    "        self.missing_features = {}  # Initialize the missing_features dictionary\n",
    "        self.train_data = self.pre_process(self.originalTrain, \"train\")  # Training data\n",
    "        self.test_data = None\n",
    "        self.params_list = params_list  # Hyperparameters to tune\n",
    "        self.best_model = None  # To store the best model after tuning\n",
    "        self.best_params = None  # To store the best parameters\n",
    "        self.n_fold_cross_validation = n_fold_cross_validation  # Number of folds for cross-validation\n",
    "        self.columns_to_keep = []  # Columns selected after pre-processing\n",
    "        self.feature_importance = {}  # Store feature importance (MI)\n",
    "\n",
    "    def map_categories_to_numeric(self, data, column_name=\"Target\"):\n",
    "        # Define the category mapping\n",
    "        category_mapping = {'low': 0, 'medium': 1, 'high': 2}\n",
    "        \n",
    "        # Map the column to numeric values\n",
    "        data[column_name] = data[column_name].map(category_mapping)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def replace_missing_values(self, data):\n",
    "        self.missing_features = {}  # Initialize dictionary to store missing values used for imputation\n",
    "\n",
    "        # Columns for mode imputation\n",
    "        mode_replaces = ['SoilFertilityType', 'ReservoirType', 'LandUsageType', 'CropSpeciesVariety',\n",
    "                    'FieldShadeCover', 'HasPestControl', 'ReservoirWithFilter', 'NaturalLakePresence',\n",
    "                    'HasGreenHouse', 'TaxOverdueStatus', 'AgricultureZoningCode', 'OtherZoningCode',\n",
    "                    'TypeOfIrrigationSystem', 'CropFieldConfiguration', 'FarmClassification',\n",
    "                    'HarvestProcessingType', 'FieldZoneLevel', 'FieldConstructionType']\n",
    "\n",
    "        \n",
    "        # Columns for mean imputation (currently empty)\n",
    "        mean_replaces = ['UndergroundStorageSqft', 'WaterAccessPoints', 'WaterAccessPointsCalc',\n",
    "                  'PrimaryCropAreaSqft', 'TotalCultivatedAreaSqft', 'CultivatedAreaSqft1',\n",
    "                  'PerimeterGuardPlantsArea', 'PrimaryCropareainsqft2', 'CultivatedAndWildArea',\n",
    "                  'TotalAreaSqft', 'NationalRegionCode', 'NumberGreenHouses', 'MainIrrigationSystemCount',\n",
    "                  'FarmVehicleCount', 'FarmEquipmentArea', 'Latitude', 'Longitude', 'FieldSizeSqft',\n",
    "                  'WaterReservoirCount', 'TotalReservoirSize', 'StorageAndFacilityCount',\n",
    "                  'PartialIrrigationSystemCount', 'FarmingUnitCount', 'FarmShedAreaSqft',\n",
    "                  'HarvestStorageSqft', 'FieldEstablishedYear', 'NumberOfFarmingZones', 'TaxAgrarianValue',\n",
    "                  'TaxLandValue', 'TotalValue', 'TotalTaxAssessed', 'ValuationYear', 'TaxOverdueYear']\n",
    "\n",
    "        \n",
    "        # Columns for KNN imputation\n",
    "        knn_impute = ['WaterAccessPointsCalc', 'WaterAccessPoints']\n",
    "\n",
    "        # Columns for MICE imputation\n",
    "        multivariate_MICE_impute = [\n",
    "            'TotalValue', 'TotalTaxAssessed', 'TotalCultivatedAreaSqft', 'TaxLandValue',\n",
    "            'TaxAgrarianValue', 'RawLocationId', 'Longitude', 'Latitude', 'CultivatedAreaSqft1',\n",
    "            'FarmEquipmentArea', 'FarmingCommunityId', 'FieldSizeSqft'\n",
    "        ]\n",
    "        def distId(ag_id):\n",
    "            if(ag_id < 20 or (ag_id>26 and ag_id<38) or (ag_id>41 and ag_id<46)):\n",
    "                return 1\n",
    "            elif ag_id>19 and ag_id<27:\n",
    "                return 2\n",
    "            else:\n",
    "                return 3\n",
    "        data['DistrictId'] = data['DistrictId'].fillna(data['AgricultureZoningCode'].apply(distId))\n",
    "\n",
    "        # Update numerical and categorical column lists after dropping columns with missing values\n",
    "        mean_replaces = [col for col in mean_replaces if col in data.columns]\n",
    "        mode_replaces = [col for col in mode_replaces if col in data.columns]\n",
    "        knn_impute = [col for col in knn_impute if col in data.columns]\n",
    "        multivariate_MICE_impute = [col for col in multivariate_MICE_impute if col in data.columns]\n",
    "        \n",
    "        # Impute using mode\n",
    "        for column in mode_replaces:\n",
    "            mode_value = data[column].mode()[0]\n",
    "            data[column] = data[column].fillna(mode_value)\n",
    "            self.missing_features[column] = mode_value  # Store mode value\n",
    "\n",
    "        # Impute using mean\n",
    "        for column in mean_replaces:\n",
    "            mean_value = data[column].mean()\n",
    "            data[column] = data[column].fillna(mean_value)\n",
    "            self.missing_features[column] = mean_value  # Store mean value\n",
    "\n",
    "        # Impute using KNN\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        data[knn_impute] = imputer.fit_transform(data[knn_impute])\n",
    "\n",
    "        # Store imputed KNN values (mean of each column used in KNN)\n",
    "        for column in knn_impute:\n",
    "            knn_mean_value = data[column].mean()\n",
    "            self.missing_features[column] = knn_mean_value\n",
    "\n",
    "        # Impute using MICE (Iterative Imputer)\n",
    "        imputer = IterativeImputer()\n",
    "        data[multivariate_MICE_impute] = imputer.fit_transform(data[multivariate_MICE_impute])\n",
    "\n",
    "        # Store imputed MICE values (mean of each column used in MICE)\n",
    "        for column in multivariate_MICE_impute:\n",
    "            mice_mean_value = data[column].mean()\n",
    "            self.missing_features[column] = mice_mean_value\n",
    "\n",
    "        # Iterate over all columns and perform additional imputations\n",
    "        for column in data.columns:\n",
    "            if column not in mode_replaces and column not in mean_replaces and \\\n",
    "            column not in knn_impute and column not in multivariate_MICE_impute:\n",
    "                \n",
    "                # Check if the column is of type float\n",
    "                if data[column].dtype == 'float64' or data[column].dtype == 'float32':\n",
    "                    # Convert floats that represent integers to int\n",
    "                    if (data[column] % 1 == 0).all():  # Check if all values in the column are effectively integers\n",
    "                        data[column] = data[column].astype('int64')  # Convert to integer type\n",
    "                    \n",
    "                    # Replace missing values in float columns with the mean\n",
    "                    mean_value = data[column].mean()\n",
    "                    data[column] = data[column].fillna(mean_value)\n",
    "                    self.missing_features[column] = mean_value  # Store mean value\n",
    "\n",
    "                # Check if the column is of type int (or was converted from float to int)\n",
    "                elif data[column].dtype == 'int64' or data[column].dtype == 'int32':\n",
    "                    # Replace missing values in integer columns with the mode\n",
    "                    mode_value = data[column].mode()[0]\n",
    "                    data[column] = data[column].fillna(mode_value)\n",
    "                    self.missing_features[column] = mode_value  # Store mode value\n",
    "\n",
    "        return data\n",
    "\n",
    "    \n",
    "# Index(['UID', 'AgriculturalPostalZone', 'AgricultureZoningCode',\n",
    "#        'CropSpeciesVariety', 'CultivatedAreaSqft1', 'DistrictId',\n",
    "#        'FarmEquipmentArea', 'FarmVehicleCount', 'FarmingCommunityId',\n",
    "#        'FarmingUnitCount', 'FieldEstablishedYear', 'FieldSizeSqft',\n",
    "#        'HarvestProcessingType', 'LandUsageType', 'Latitude', 'Longitude',\n",
    "#        'MainIrrigationSystemCount', 'NationalRegionCode',\n",
    "#        'NaturalLakePresence', 'NumberOfFarmingZones', 'OtherZoningCode',\n",
    "#        'RawLocationId', 'SoilFertilityType', 'StorageAndFacilityCount',\n",
    "#        'TaxAgrarianValue', 'TaxLandValue', 'TotalCultivatedAreaSqft',\n",
    "#        'TotalTaxAssessed', 'TotalValue', 'TownId', 'TypeOfIrrigationSystem',\n",
    "#        'ValuationYear', 'WaterAccessPoints', 'WaterAccessPointsCalc',\n",
    "#        'WaterReservoirCount', 'Target'],\n",
    "#       dtype='object')\n",
    "\n",
    "\n",
    "    def pre_process(self, data, dataType=\"train\"):\n",
    "        if dataType == \"train\":\n",
    "            data = self.map_categories_to_numeric(data)\n",
    "\n",
    "            # Drop columns with more than a certain threshold of missing values\n",
    "            threshold = 0.67  # 90% missing values\n",
    "            threshold_count = int((1 - threshold) * len(data))\n",
    "            data = data.dropna(axis=1, thresh=threshold_count)\n",
    "\n",
    "\n",
    "            # Columns to be One-Hot Encoded\n",
    "            oneHotEncoding = [\n",
    "                'CropSpeciesVariety', 'DistrictId', \n",
    "                'FieldEstablishedYear', 'HarvestProcessingType', \n",
    "                'LandUsageType', 'NationalRegionCode', 'SoilFertilityType', \n",
    "                'TypeOfIrrigationSystem', 'ValuationYear',\n",
    "            ]\n",
    "            oneHotEncoding = [col for col in oneHotEncoding if col in data.columns]\n",
    "            # Replace missing values in columns\n",
    "            data = self.replace_missing_values(data)\n",
    "            \n",
    "            # Perform One-Hot Encoding\n",
    "            data = pd.get_dummies(data, columns=oneHotEncoding, drop_first=True)\n",
    "\n",
    "            # Remove irrelevant columns based on feature-target relationship\n",
    "            data = self.remove_irrelevant_columns(data)\n",
    "            \n",
    "            # Drop the 'UID' column, as it's not needed for training\n",
    "            if('UID' in data.columns):\n",
    "                data = data.drop(columns=['UID'])\n",
    "            \n",
    "            self.columns_to_keep = data.columns\n",
    "\n",
    "            for col in self.columns_to_keep:\n",
    "                if col not in self.missing_features.keys():\n",
    "                    self.missing_features[col] = 0\n",
    "        elif dataType == \"test\":\n",
    "            # Ensure all columns in self.columns_to_keep are in the test set\n",
    "            missing_columns = [col for col in self.columns_to_keep if col not in data.columns]\n",
    "            if missing_columns:\n",
    "                for col in missing_columns:\n",
    "                    data[col] = pd.NA  # Add missing columns with NaN values\n",
    "\n",
    "            # Drop any extra columns not in self.columns_to_keep\n",
    "            extra_columns = [col for col in data.columns if col not in self.columns_to_keep]\n",
    "            if extra_columns:\n",
    "                data = data.drop(columns=extra_columns)\n",
    "\n",
    "            # Reindex columns to match the training data\n",
    "            data = data.reindex(columns=self.columns_to_keep)\n",
    "\n",
    "            # Replace missing values using the previously imputed values\n",
    "            for column in data.columns:\n",
    "                data[column] = data[column].fillna(self.missing_features[column])\n",
    "            # Drop 'Target' column as it's not used for predictions\n",
    "            if('Target' in data.columns):\n",
    "                data = data.drop(columns='Target')\n",
    "            \n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "    # def remove_irrelevant_columns(self, data):\n",
    "    #     # Separate features and target\n",
    "    #     X = data.drop('Target', axis=1)\n",
    "    #     y = data['Target']\n",
    "        \n",
    "    #     # Train a Random Forest model to determine feature importances\n",
    "    #     model = RandomForestClassifier(random_state=42)\n",
    "    #     model.fit(X, y)\n",
    "\n",
    "    #     # Get feature importances\n",
    "    #     importances = model.feature_importances_\n",
    "\n",
    "    #     # Create a DataFrame to display feature importances\n",
    "    #     feature_importance_df = pd.DataFrame({\n",
    "    #         'Feature': X.columns,\n",
    "    #         'Importance': importances\n",
    "    #     }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    #     print(\"Feature Importances:\\n\", feature_importance_df)\n",
    "\n",
    "    #     # Determine columns to remove based on importance threshold\n",
    "    #     threshold = 0.01  # Adjust the threshold as needed\n",
    "    #     irrelevant_columns = feature_importance_df[feature_importance_df['Importance'] < threshold]['Feature'].tolist()\n",
    "        \n",
    "    #     print(f\"Removing columns with low importance: {irrelevant_columns}\")\n",
    "        \n",
    "    #     # Drop irrelevant columns\n",
    "    #     data = data.drop(columns=irrelevant_columns)\n",
    "\n",
    "    #     return data\n",
    "    \n",
    "    # def remove_irrelevant_columns(self, data):\n",
    "    #     # Assuming 'data' is your DataFrame and 'Target' is the name of your target variable\n",
    "    #     X = data.drop('Target', axis=1)\n",
    "    #     y = data['Target']\n",
    "\n",
    "    #     # Initialize an empty list to collect feature correlation dictionaries\n",
    "    #     correlation_list = []\n",
    "\n",
    "    #     for column in X.columns:\n",
    "    #         # Calculate Pearson correlation (only if y is continuous)\n",
    "    #         pearson_corr, _ = pearsonr(X[column], y)\n",
    "    #         # Calculate Spearman correlation\n",
    "    #         spearman_corr, _ = spearmanr(X[column], y)\n",
    "            \n",
    "    #         # Append to the list as a dictionary\n",
    "    #         correlation_list.append({\n",
    "    #             'Feature': column,\n",
    "    #             'Pearson': pearson_corr,\n",
    "    #             'Spearman': spearman_corr\n",
    "    #         })\n",
    "\n",
    "    #     # Create a DataFrame from the list\n",
    "    #     correlation_df = pd.DataFrame(correlation_list)\n",
    "\n",
    "    #     # Sort features by the absolute value of their Spearman correlations\n",
    "    #     correlation_df['Abs_Pearson'] = correlation_df['Pearson'].abs()\n",
    "    #     correlation_df['Abs_Spearman'] = correlation_df['Spearman'].abs()\n",
    "    #     correlation_df = correlation_df.sort_values(by='Abs_Pearson ', ascending=False)\n",
    "\n",
    "    #     # Select the top 30 features\n",
    "    #     top_features = correlation_df['Feature'].head(30).tolist()\n",
    "\n",
    "    #     # Keep only the top 30 features in the original data\n",
    "    #     data = data[top_features + ['Target']]\n",
    "\n",
    "\n",
    "    #     print(\"Top 30 features based on absolute Spearman correlation:\\n\", top_features)\n",
    "\n",
    "    #     return data\n",
    "\n",
    "\n",
    "\n",
    "    def remove_irrelevant_columns(self, data):\n",
    "        correlation_threshold=0.01\n",
    "        if('Target' in data.columns):\n",
    "            correlation_matrix = data.corr()\n",
    "            low_corr_columns = correlation_matrix['Target'].apply(lambda x: abs(x) < correlation_threshold)\n",
    "            columns_to_drop = correlation_matrix.columns[low_corr_columns].tolist()\n",
    "\n",
    "            data = data.drop(columns=columns_to_drop)\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def best_model_outputer(self):\n",
    "        # Define the custom scoring function (F1 score)\n",
    "        f1_scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "        # Perform Grid Search with Stratified K-Fold and F1 score as the metric\n",
    "        grid_search = GridSearchCV(\n",
    "            self.model, \n",
    "            self.params_list, \n",
    "            cv=self.n_fold_cross_validation, \n",
    "            scoring=f1_scorer, \n",
    "            n_jobs=-2, \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # # Apply RandomUnderSampler for undersampling\n",
    "        # undersampler = RandomUnderSampler(random_state=42)\n",
    "        # X_resampled, y_resampled = undersampler.fit_resample(\n",
    "        #     self.train_data.drop('Target', axis=1), \n",
    "        #     self.train_data['Target']\n",
    "        # )\n",
<<<<<<< Updated upstream
    "\n",
    "        # # Apply NearMiss-2 for undersampling\n",
    "        # nearmiss = NearMiss(version=2)\n",
    "        # X_resampled, y_resampled = nearmiss.fit_resample(\n",
    "        #     self.train_data.drop('Target', axis=1), \n",
    "        #     self.train_data['Target']\n",
    "        # )\n",
    "\n",
    "        # Apply Tomek Links for undersampling\n",
    "        tomek_links = TomekLinks()\n",
    "        X_resampled, y_resampled = tomek_links.fit_resample(\n",
    "            self.train_data.drop('Target', axis=1), \n",
    "            self.train_data['Target']\n",
    "        )\n",
    "\n",
=======
    "        X_resampled = self.train_data.drop('Target', axis=1)\n",
    "        y_resampled = self.train_data['Target']\n",
>>>>>>> Stashed changes
    "        # Use the entire training data for hyperparameter tuning\n",
    "        grid_search.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        # Store the best model and best parameters\n",
    "        self.best_model = grid_search.best_estimator_\n",
    "        self.best_params = grid_search.best_params_\n",
    "\n",
    "        print(f\"Best parameters: {self.best_params}\")\n",
    "        print(f\"Best F1 score from Grid Search: {grid_search.best_score_}\")\n",
    "\n",
    "        # Perform cross-validation with the best model\n",
    "        kf = StratifiedKFold(n_splits=self.n_fold_cross_validation, shuffle=True, random_state=42)\n",
    "        f1_scores_macro = []\n",
    "\n",
    "        # Cross-validation loop to calculate the macro F1 score for each fold\n",
    "        for train_index, test_index in kf.split(X_resampled, y_resampled):\n",
    "            X_train, X_test = X_resampled.iloc[train_index], X_resampled.iloc[test_index]\n",
    "            y_train, y_test = y_resampled.iloc[train_index], y_resampled.iloc[test_index]\n",
    "            \n",
    "            # Train the model on the training set\n",
    "            self.best_model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict on the test set\n",
    "            y_pred = self.best_model.predict(X_test)\n",
    "            \n",
    "            # Calculate the macro F1 score for this fold\n",
    "            f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "            f1_scores_macro.append(f1_macro)\n",
    "        \n",
    "        # Calculate the mean and standard deviation of the macro F1 scores\n",
    "        mean_f1_macro = np.mean(f1_scores_macro)\n",
    "        std_f1_macro = np.std(f1_scores_macro)\n",
    "\n",
    "        print(f\"\\n\\nCross-validation results:\")\n",
    "        print(f\"\\tMacro F1 Scores: {f1_scores_macro}\")\n",
    "        print(f\"\\tMean Macro F1 Score: {mean_f1_macro:.4f}\")\n",
    "        print(f\"\\tStandard Deviation of Macro F1 Score: {std_f1_macro:.4f}\")\n",
    "\n",
    "        return self.best_model, self.best_params\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        # Generate predictions using the best model\n",
    "        predictions = self.best_model.predict(test_data)\n",
    "        \n",
    "        # Define the reverse mapping from numeric to category\n",
    "        reverse_mapping = {0: 'low', 1: 'medium', 2: 'high'}\n",
    "        \n",
    "        # Map numeric predictions back to categories\n",
    "        return pd.Series(predictions).map(reverse_mapping)\n",
    "\n",
    "\n",
    "    def make_predictions(self, test_fname, predictions_fname):\n",
    "        # Save the columns to keep for consistency with the test set\n",
    "        self.columns_to_keep = list(self.train_data.columns)  # Save as a list to maintain order\n",
    "        print(\"Columns to keep:\", self.columns_to_keep)\n",
    "\n",
    "        test_data = pd.read_csv(test_fname)\n",
    "        self.originalTest = test_data\n",
    "        self.test_data = self.pre_process(data=test_data, dataType=\"test\")\n",
    "        predictions = self.predict(self.test_data)\n",
    "\n",
    "        # Step 4: Add UID column from `copy_test` DataFrame to the `reversed_predictions`\n",
    "        reversed_predictions_df = pd.DataFrame({\n",
    "            'UID': self.originalTest['UID'],\n",
    "            'Target': predictions  # The reversed prediction values\n",
    "        })\n",
    "        reversed_predictions_df.to_csv(predictions_fname, index=False)\n",
    "        print(\"File Made and prediction complete\")\n",
    "        return reversed_predictions_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost():\n",
    "    # Usage Example\n",
    "    # Assuming train_data and test_data are pandas DataFrames, and model is an estimator like XGBoost\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    # Example params list (you can adjust this for your model)\n",
    "    params_list = {\n",
    "        'learning_rate': [0.01],\n",
    "        'n_estimators': [100, 150, 200],\n",
    "        'max_depth': [7, 9]\n",
    "    }\n",
    "\n",
    "\n",
    "    # Assuming `train_data` and `test_data` are already defined\n",
    "    model = XGBClassifier(random_state=42)\n",
    "\n",
    "    # Create an instance of bestModel\n",
    "    best_model_instance = BestModel(model, \"train.csv\", params_list)\n",
    "\n",
    "    # Get the best model and its parameters\n",
    "    best_model, best_params = best_model_instance.best_model_outputer()\n",
    "\n",
    "    print(f\"Best Model of xGBoost with params : {best_params}\")\n",
    "    best_model_instance.make_predictions('test.csv','submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm():\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    # Example hyperparameter list for SVM with RBF kernel\n",
    "    params_list = {\n",
    "        'C': [0.1, 1, 10],  # Regularization parameter\n",
    "        'gamma': ['scale', 'auto', 0.1, 1],  # Kernel coefficient\n",
    "        'kernel': ['rbf']  # RBF kernel\n",
    "    }\n",
    "\n",
    "    # Create an SVM model instance\n",
    "    model = SVC(random_state=42)\n",
    "\n",
    "    # Create an instance of BestModel\n",
    "    best_model_instance = BestModel(model, \"train.csv\", params_list)\n",
    "\n",
    "    # Get the best model and its parameters\n",
    "    best_model, best_params = best_model_instance.best_model_outputer()\n",
    "\n",
    "    print(f\"Best Model of SVM with rbf kernal with params : {best_params}\")\n",
    "    best_model_instance.make_predictions('test.csv','submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest():\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    # Example hyperparameter list for Random Forest\n",
    "    params_list = {\n",
    "        'n_estimators': [100],  # Number of trees in the forest\n",
    "        'max_depth': [None],  # Maximum depth of the tree\n",
    "        'min_samples_split': [2],  # Minimum number of samples required to split a node\n",
    "        'max_features': ['sqrt'],\n",
    "        'class_weight': ['balanced']  # Adding class weights to handle imbalanced classes\n",
    "    }\n",
    "\n",
    "    # Create a Random Forest model instance with class weights\n",
    "    model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "    # Create an instance of BestModel\n",
    "    best_model_instance = BestModel(model, \"/home/whitedevil/Desktop/Education/FoML/Kaggle_Hackathon/ML_Kaggle_hackathon/train.csv\", params_list)\n",
    "\n",
    "    # Get the best model and its parameters\n",
    "    best_model, best_params = best_model_instance.best_model_outputer()\n",
    "\n",
    "    print(f\"Best Model of Random Forest with params: {best_params}\")\n",
    "    best_model_insance_global, best_model_global = best_model_instance, best_model\n",
    "    best_model_instance.make_predictions('/home/whitedevil/Desktop/Education/FoML/Kaggle_Hackathon/ML_Kaggle_hackathon/test.csv', 'submission.csv')\n",
    "\n",
    "    return best_model, best_model_instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn():\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    # Example hyperparameter list for KNN\n",
    "    params_list = {\n",
    "        'n_neighbors': [3],  # Number of neighbors\n",
    "        'weights': ['distance'],  # Weight function used in prediction\n",
    "        'metric': ['manhattan']  # Distance metrics\n",
    "    }\n",
    "\n",
    "    # Create a KNN model instance\n",
    "    model = KNeighborsClassifier()\n",
    "\n",
    "    # Create an instance of BestModel\n",
    "    best_model_instance = BestModel(model, \"train.csv\", params_list)\n",
    "\n",
    "    # Get the best model and its parameters\n",
    "    best_model, best_params = best_model_instance.best_model_outputer()\n",
    "\n",
    "    print(f\"Best Model of KNN with params: {best_params}\")\n",
    "    best_model_instance.make_predictions('test.csv', 'submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_lstm():\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Load your training data\n",
    "    train_data = pd.read_csv(\"train.csv\")\n",
    "    X = train_data.iloc[:, :-1].values  # Assuming features are all columns except the last\n",
    "    y = train_data.iloc[:, -1].values   # Assuming the last column is the target\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Reshape the data for LSTM (samples, timesteps, features)\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_val = np.expand_dims(X_val, axis=2)\n",
    "\n",
    "    # Build a simple Bi-LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(Bidirectional(LSTM(32)))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Use 'softmax' for multi-class classification\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
    "\n",
    "    # Save predictions for the test data\n",
    "    test_data = pd.read_csv(\"test.csv\")\n",
    "    X_test = test_data.values\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Save predictions to submission.csv\n",
    "    pd.DataFrame(predictions, columns=[\"Predictions\"]).to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "    print(\"Bi-LSTM model training and predictions completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression():\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    # Example hyperparameter list for Linear Regression\n",
    "    params_list = {\n",
    "        'fit_intercept': [True, False],  # Whether to calculate the intercept for this model\n",
    "        'normalize': [True, False],  # Whether to normalize the data before fitting\n",
    "    }\n",
    "\n",
    "    # Create a Linear Regression model instance\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Create an instance of BestModel\n",
    "    best_model_instance = BestModel(model, \"train.csv\", params_list)\n",
    "\n",
    "    # Get the best model and its parameters\n",
    "    best_model, best_params = best_model_instance.best_model_outputer()\n",
    "\n",
    "    print(f\"Best Model of Linear Regression with params: {best_params}\")\n",
    "    best_model_instance.make_predictions('test.csv', 'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    # Example hyperparameter list for Logistic Regression\n",
    "    params_list = {\n",
    "        'C': [0.1, 1, 10],  # Regularization strength\n",
    "        'penalty': ['l2'],  # Regularization type\n",
    "        'solver': ['lbfgs', 'liblinear'],  # Optimization algorithm\n",
    "        'max_iter': [100, 200]  # Maximum number of iterations for the solver\n",
    "    }\n",
    "\n",
    "    # Create a Logistic Regression model instance\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Create an instance of BestModel\n",
    "    best_model_instance = BestModel(model, \"train.csv\", params_list)\n",
    "\n",
    "    # Get the best model and its parameters\n",
    "    best_model, best_params = best_model_instance.best_model_outputer()\n",
    "\n",
    "    print(f\"Best Model of Logistic Regression with params: {best_params}\")\n",
    "    best_model_instance.make_predictions('test.csv', 'submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost_DT():\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    # Example params list for AdaBoost\n",
    "    params_list = {\n",
    "        'n_estimators': [50, 100, 150],  # Number of boosting rounds\n",
    "        'learning_rate': [0.01, 0.1, 1.0]  # Learning rate\n",
    "    }\n",
    "\n",
    "    # Initialize a weak classifier (e.g., DecisionTreeClassifier)\n",
    "    base_model = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "    # Create an AdaBoost model instance\n",
    "    model = AdaBoostClassifier(estimator=base_model, random_state=42)\n",
    "\n",
    "    # Create an instance of BestModel\n",
    "    best_model_instance = BestModel(model, \"train.csv\", params_list)\n",
    "\n",
    "    # Get the best model and its parameters\n",
    "    best_model, best_params = best_model_instance.best_model_outputer()\n",
    "\n",
    "    print(f\"Best Model of AdaBoost with params: {best_params}\")\n",
    "    best_model_instance.make_predictions('test.csv', 'submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "/tmp/ipykernel_129662/1651871533.py:54: SettingWithCopyWarning: \n",
=======
      "/tmp/ipykernel_144420/1614435184.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['DistrictId'] = data['DistrictId'].fillna(data['AgricultureZoningCode'].apply(distId))\n",
      "/tmp/ipykernel_144420/1614435184.py:75: SettingWithCopyWarning: \n",
>>>>>>> Stashed changes
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].fillna(mode_value)\n",
<<<<<<< Updated upstream
      "/tmp/ipykernel_129662/1651871533.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].fillna(mode_value)\n",
      "/tmp/ipykernel_129662/1651871533.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[knn_impute] = imputer.fit_transform(data[knn_impute])\n",
      "/home/yaswanth/.local/lib/python3.10/site-packages/sklearn/impute/_iterative.py:825: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_129662/1651871533.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[multivariate_MICE_impute] = imputer.fit_transform(data[multivariate_MICE_impute])\n",
      "/tmp/ipykernel_129662/1651871533.py:101: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].fillna(mode_value)\n",
      "/tmp/ipykernel_129662/1651871533.py:94: SettingWithCopyWarning: \n",
=======
      "/tmp/ipykernel_144420/1614435184.py:81: SettingWithCopyWarning: \n",
>>>>>>> Stashed changes
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].fillna(mean_value)\n",
<<<<<<< Updated upstream
      "/tmp/ipykernel_129662/1651871533.py:101: SettingWithCopyWarning: \n",
=======
      "/tmp/ipykernel_144420/1614435184.py:86: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[knn_impute] = imputer.fit_transform(data[knn_impute])\n",
      "/tmp/ipykernel_144420/1614435184.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[multivariate_MICE_impute] = imputer.fit_transform(data[multivariate_MICE_impute])\n",
      "/tmp/ipykernel_144420/1614435184.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].fillna(mode_value)\n",
      "/tmp/ipykernel_144420/1614435184.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].fillna(mode_value)\n",
      "/tmp/ipykernel_144420/1614435184.py:111: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].astype('int64')  # Convert to integer type\n",
      "/tmp/ipykernel_144420/1614435184.py:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].fillna(mean_value)\n",
      "/tmp/ipykernel_144420/1614435184.py:122: SettingWithCopyWarning: \n",
>>>>>>> Stashed changes
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].fillna(mode_value)\n"
     ]
    },
    {
<<<<<<< Updated upstream
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Best parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best F1 score from Grid Search: 0.39332980505094667\n",
      "\n",
      "\n",
      "Cross-validation results:\n",
      "\tMacro F1 Scores: [0.4004827949155981, 0.3896790867620667, 0.390841214359269, 0.39978442946413456, 0.39048661888158853, 0.4041657403186571, 0.39882239697405303, 0.3966668723730518, 0.38205084961923347, 0.3907459636250503]\n",
      "\tMean Macro F1 Score: 0.3944\n",
      "\tStandard Deviation of Macro F1 Score: 0.0063\n",
      "Best Model of Random Forest with params: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Columns to keep: ['FarmVehicleCount', 'Latitude', 'Longitude', 'MainIrrigationSystemCount', 'NaturalLakePresence', 'NumberGreenHouses', 'NumberOfFarmingZones', 'RawLocationId', 'StorageAndFacilityCount', 'TaxLandValue', 'TotalCultivatedAreaSqft', 'TotalTaxAssessed', 'TownId', 'WaterAccessPoints', 'WaterAccessPointsCalc', 'WaterReservoirCount', 'Target', 'CropSpeciesVariety_2.0', 'CropSpeciesVariety_3.0', 'CropSpeciesVariety_4.0', 'CropSpeciesVariety_5.0', 'CropSpeciesVariety_6.0', 'DistrictId_2.0', 'FieldEstablishedYear_1928.0', 'FieldEstablishedYear_1931.0', 'FieldEstablishedYear_1955.0', 'FieldEstablishedYear_1958.0', 'FieldEstablishedYear_1959.0', 'FieldEstablishedYear_2007.0', 'FieldEstablishedYear_2008.0', 'FieldEstablishedYear_2009.0', 'FieldEstablishedYear_2010.0', 'FieldEstablishedYear_2011.0', 'HarvestProcessingType_2.0', 'NationalRegionCode_2.0', 'SoilFertilityType_9.0', 'SoilFertilityType_10.0', 'SoilFertilityType_11.0', 'TypeOfIrrigationSystem_4.0', 'ValuationYear_2020.0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129662/1651871533.py:172: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[column] = data[column].fillna(self.missing_features[column])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Made and prediction complete\n"
=======
     "ename": "InvalidParameterError",
     "evalue": "The 'cv' parameter of GridSearchCV must be an int in the range [2, inf), an object implementing 'split' and 'get_n_splits', an iterable or None. Got 1 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# xgboost()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# svm()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# knn()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m best_model, best_model_instance\u001b[38;5;241m=\u001b[39m\u001b[43mrandom_forest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# bi_lstm\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# logistic_regression()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# linear_regression()\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# adaboost_DT()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m, in \u001b[0;36mrandom_forest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m best_model_instance \u001b[38;5;241m=\u001b[39m BestModel(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/whitedevil/Desktop/Education/FoML/Kaggle_Hackathon/ML_Kaggle_hackathon/train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, params_list)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Get the best model and its parameters\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m best_model, best_params \u001b[38;5;241m=\u001b[39m \u001b[43mbest_model_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_model_outputer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Model of Random Forest with params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m best_model_insance_global, best_model_global \u001b[38;5;241m=\u001b[39m best_model_instance, best_model\n",
      "Cell \u001b[0;32mIn[3], line 314\u001b[0m, in \u001b[0;36mBestModel.best_model_outputer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m y_resampled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# Use the entire training data for hyperparameter tuning\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_resampled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# Store the best model and best parameters\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m~/workspace/Python Enviroments/general/lib/python3.12/site-packages/sklearn/base.py:1466\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1461\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1462\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[1;32m   1463\u001b[0m )\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[0;32m-> 1466\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[1;32m   1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/workspace/Python Enviroments/general/lib/python3.12/site-packages/sklearn/base.py:666\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    659\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \n\u001b[1;32m    661\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 666\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/Python Enviroments/general/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m     )\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'cv' parameter of GridSearchCV must be an int in the range [2, inf), an object implementing 'split' and 'get_n_splits', an iterable or None. Got 1 instead."
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "# xgboost()\n",
    "# svm()\n",
    "# knn()\n",
    "best_model, best_model_instance=random_forest()\n",
    "# bi_lstm\n",
    "# logistic_regression()\n",
    "# linear_regression()\n",
    "# adaboost_DT()\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 13,
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
