{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "train_path = 'train.csv'\n",
    "test_path = 'test.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "train_data = pd.read_csv(train_path)\n",
    "test_data = pd.read_csv(test_path)\n",
    "\n",
    "print(f\"{train_data.columns}, Length = {len(train_data.columns)}\")\n",
    "data = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestModel:\n",
    "    def __init__(self, model=None, train_data_fname=\"'train.csv'\", params_list=None, n_fold_cross_validation=10):\n",
    "        self.model = model  # The model to be tuned\n",
    "        self.originalTrain = pd.read_csv(train_data_fname)\n",
    "        self.originalTest = None\n",
    "        self.missing_features = {}  # Initialize the missing_features dictionary\n",
    "        self.train_data = self.pre_process(self.originalTrain, \"train\")  # Training data\n",
    "        self.test_data = None\n",
    "        self.params_list = params_list  # Hyperparameters to tune\n",
    "        self.best_model = None  # To store the best model after tuning\n",
    "        self.best_params = None  # To store the best parameters\n",
    "        self.n_fold_cross_validation = n_fold_cross_validation  # Number of folds for cross-validation\n",
    "        self.columns_to_keep = []  # Columns selected after pre-processing\n",
    "        self.feature_importance = {}  # Store feature importance (MI)\n",
    "\n",
    "    def map_categories_to_numeric(self, data, column_name=\"Target\"):\n",
    "        # Define the category mapping\n",
    "        category_mapping = {'low': 0, 'medium': 1, 'high': 2}\n",
    "        \n",
    "        # Map the column to numeric values\n",
    "        data[column_name] = data[column_name].map(category_mapping)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def replace_missing_values(self, data):\n",
    "        mode_replaces = ['WaterReservoirCount', 'ValuationYear', 'TypeOfIrrigationSystem', 'TownId', 'StorageAndFacilityCount', 'SoilFertilityType', 'OtherZoningCode', 'NumberOfFarmingZones', 'NaturalLakePresence', 'NationalRegionCode', 'MainIrrigationSystemCount', 'LandUsageType', 'HarvestProcessingType', 'AgriculturalPostalZone', 'CropSpeciesVariety', 'DistrictId', 'FarmingUnitCount', 'FieldEstablishedYear']\n",
    "        mean_replaces = ['WaterAccessPointsCalc', 'WaterAccessPoints', 'TotalValue', 'TotalTaxAssessed', 'TotalCultivatedAreaSqft', 'TaxLandValue', 'TaxAgrarianValue', 'RawLocationId', 'Longitude', 'Latitude', 'CultivatedAreaSqft1', 'FarmEquipmentArea', 'FarmingCommunityId', 'FieldSizeSqft']\n",
    "        \n",
    "        for column in mode_replaces:\n",
    "            mode_value = data[column].mode()[0]\n",
    "            data[column] = data[column].fillna(mode_value) \n",
    "            self.missing_features[column] = mode_value\n",
    "        for column in mean_replaces:\n",
    "            mean_value = data[column].mean()\n",
    "            data[column] = data[column].fillna(mean_value)  # Fixing the warning by avoiding inplace=True\n",
    "            self.missing_features[column] = mean_value\n",
    "        # Iterate over each column and replace missing values\n",
    "        for column in data.columns:\n",
    "            if(column not in mode_replaces and column not in mean_replaces):\n",
    "                # Check if the column is of type float\n",
    "                if data[column].dtype == 'float64' or data[column].dtype == 'float32':\n",
    "                    # Convert floats that represent integers to int\n",
    "                    if (data[column] % 1 == 0).all():  # Check if all values in the column are effectively integers\n",
    "                        data[column] = data[column].astype('int64')  # Convert to integer type\n",
    "\n",
    "                    # Replace missing values in float columns with the mean\n",
    "                    mean_value = data[column].mean()\n",
    "                    data[column] = data[column].fillna(mean_value)  # Fixing the warning by avoiding inplace=True\n",
    "                    self.missing_features[column] = mean_value\n",
    "\n",
    "                # Check if the column is of type int (or was converted from float to int)\n",
    "                elif data[column].dtype == 'int64' or data[column].dtype == 'int32':\n",
    "                    # Replace missing values in integer columns with the mode\n",
    "                    mode_value = data[column].mode()[0]\n",
    "                    data[column] = data[column].fillna(mode_value)  # Fixing the warning by avoiding inplace=True\n",
    "                    self.missing_features[column] = mode_value\n",
    "\n",
    "        return data\n",
    "    \n",
    "# Index(['UID', 'AgriculturalPostalZone', 'AgricultureZoningCode',\n",
    "#        'CropSpeciesVariety', 'CultivatedAreaSqft1', 'DistrictId',\n",
    "#        'FarmEquipmentArea', 'FarmVehicleCount', 'FarmingCommunityId',\n",
    "#        'FarmingUnitCount', 'FieldEstablishedYear', 'FieldSizeSqft',\n",
    "#        'HarvestProcessingType', 'LandUsageType', 'Latitude', 'Longitude',\n",
    "#        'MainIrrigationSystemCount', 'NationalRegionCode',\n",
    "#        'NaturalLakePresence', 'NumberOfFarmingZones', 'OtherZoningCode',\n",
    "#        'RawLocationId', 'SoilFertilityType', 'StorageAndFacilityCount',\n",
    "#        'TaxAgrarianValue', 'TaxLandValue', 'TotalCultivatedAreaSqft',\n",
    "#        'TotalTaxAssessed', 'TotalValue', 'TownId', 'TypeOfIrrigationSystem',\n",
    "#        'ValuationYear', 'WaterAccessPoints', 'WaterAccessPointsCalc',\n",
    "#        'WaterReservoirCount', 'Target'],\n",
    "#       dtype='object')\n",
    "\n",
    "\n",
    "    def pre_process(self, data, dataType=\"train\"):\n",
    "        if dataType == \"train\":\n",
    "            data = self.map_categories_to_numeric(data)\n",
    "            \n",
    "            # Drop columns with more than a certain threshold of missing values\n",
    "            threshold = 0.85  # 80% missing values\n",
    "            threshold_count = int((1 - threshold) * len(data))\n",
    "            data = data.dropna(axis=1, thresh=threshold_count)\n",
    "\n",
    "            print(data.columns)\n",
    "\n",
    "            # Replace missing values in columns\n",
    "            data = self.replace_missing_values(data)\n",
    "\n",
    "            # Remove irrelevant columns based on feature-target relationship\n",
    "            data = self.remove_irrelevant_columns(data)\n",
    "\n",
    "            data = data.drop(columns=['UID'])\n",
    "\n",
    "\n",
    "        elif dataType == \"test\":\n",
    "            # Ensure all columns in self.columns_to_keep are in the test set\n",
    "            missing_columns = [col for col in self.columns_to_keep if col not in data.columns]\n",
    "            if missing_columns:\n",
    "                # Add missing columns to the test data with NaN values\n",
    "                for col in missing_columns:\n",
    "                    data[col] = pd.NA  # You can replace `pd.NA` with other placeholder values if needed\n",
    "                # print(f\"Added missing columns to the test data: {missing_columns}\")\n",
    "\n",
    "            # Drop any extra columns that are not in self.columns_to_keep\n",
    "            extra_columns = [col for col in data.columns if col not in self.columns_to_keep]\n",
    "            if extra_columns:\n",
    "                data = data.drop(columns=extra_columns)\n",
    "                # print(f\"Dropped extra columns from the test data: {extra_columns}\")\n",
    "\n",
    "\n",
    "            # Reindex columns in the same order as the training data\n",
    "            data = data.reindex(columns=self.columns_to_keep)\n",
    "\n",
    "            # Replace missing values in the test data\n",
    "            # data = self.replace_missing_values(data)\n",
    "            for column in data.columns:\n",
    "                data[column] = data[column].fillna(self.missing_features[column])\n",
    "            data = data.drop(columns='Target')\n",
    "            \n",
    "            # print(f\"Final test data columns: {data.columns}\")\n",
    "\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    # def remove_irrelevant_columns(self, data):\n",
    "    #     # Separate features and target\n",
    "    #     X = data.drop('Target', axis=1)\n",
    "    #     y = data['Target']\n",
    "        \n",
    "    #     # Train a Random Forest model to determine feature importances\n",
    "    #     model = RandomForestClassifier(random_state=42)\n",
    "    #     model.fit(X, y)\n",
    "\n",
    "    #     # Get feature importances\n",
    "    #     importances = model.feature_importances_\n",
    "\n",
    "    #     # Create a DataFrame to display feature importances\n",
    "    #     feature_importance_df = pd.DataFrame({\n",
    "    #         'Feature': X.columns,\n",
    "    #         'Importance': importances\n",
    "    #     }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    #     print(\"Feature Importances:\\n\", feature_importance_df)\n",
    "\n",
    "    #     # Determine columns to remove based on importance threshold\n",
    "    #     threshold = 0.01  # Adjust the threshold as needed\n",
    "    #     irrelevant_columns = feature_importance_df[feature_importance_df['Importance'] < threshold]['Feature'].tolist()\n",
    "        \n",
    "    #     print(f\"Removing columns with low importance: {irrelevant_columns}\")\n",
    "        \n",
    "    #     # Drop irrelevant columns\n",
    "    #     data = data.drop(columns=irrelevant_columns)\n",
    "\n",
    "    #     return data\n",
    "    \n",
    "    # def remove_irrelevant_columns(self, data):\n",
    "    #     # Assuming 'data' is your DataFrame and 'Target' is the name of your target variable\n",
    "    #     X = data.drop('Target', axis=1)\n",
    "    #     y = data['Target']\n",
    "\n",
    "    #     # Initialize an empty list to collect feature correlation dictionaries\n",
    "    #     correlation_list = []\n",
    "\n",
    "    #     for column in X.columns:\n",
    "    #         # Calculate Pearson correlation (only if y is continuous)\n",
    "    #         pearson_corr, _ = pearsonr(X[column], y)\n",
    "    #         # Calculate Spearman correlation\n",
    "    #         spearman_corr, _ = spearmanr(X[column], y)\n",
    "            \n",
    "    #         # Append to the list as a dictionary\n",
    "    #         correlation_list.append({\n",
    "    #             'Feature': column,\n",
    "    #             'Pearson': pearson_corr,\n",
    "    #             'Spearman': spearman_corr\n",
    "    #         })\n",
    "\n",
    "    #     # Create a DataFrame from the list\n",
    "    #     correlation_df = pd.DataFrame(correlation_list)\n",
    "\n",
    "    #     # Sort features by the absolute value of their Spearman correlations\n",
    "    #     correlation_df['Abs_Pearson'] = correlation_df['Pearson'].abs()\n",
    "    #     correlation_df['Abs_Spearman'] = correlation_df['Spearman'].abs()\n",
    "    #     correlation_df = correlation_df.sort_values(by='Abs_Pearson ', ascending=False)\n",
    "\n",
    "    #     # Select the top 30 features\n",
    "    #     top_features = correlation_df['Feature'].head(30).tolist()\n",
    "\n",
    "    #     # Keep only the top 30 features in the original data\n",
    "    #     data = data[top_features + ['Target']]\n",
    "\n",
    "\n",
    "    #     print(\"Top 30 features based on absolute Spearman correlation:\\n\", top_features)\n",
    "\n",
    "    #     return data\n",
    "\n",
    "\n",
    "\n",
    "    def remove_irrelevant_columns(self, data):\n",
    "        correlation_threshold=0.01\n",
    "        if('Target' in data.columns):\n",
    "            correlation_matrix = data.corr()\n",
    "            low_corr_columns = correlation_matrix['Target'].apply(lambda x: abs(x) < correlation_threshold)\n",
    "            columns_to_drop = correlation_matrix.columns[low_corr_columns].tolist()\n",
    "\n",
    "            data = data.drop(columns=columns_to_drop)\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "    def best_model_outputer(self):\n",
    "        # Define the custom scoring function (F1 score)\n",
    "        f1_scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "        # Perform Grid Search with Stratified K-Fold and F1 score as the metric\n",
    "        grid_search = GridSearchCV(\n",
    "            self.model, \n",
    "            self.params_list, \n",
    "            cv=self.n_fold_cross_validation, \n",
    "            scoring=f1_scorer, \n",
    "            n_jobs=-2, \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Use the entire training data for hyperparameter tuning\n",
    "        grid_search.fit(self.train_data.drop('Target', axis=1), self.train_data['Target'])\n",
    "        \n",
    "        # Store the best model and best parameters\n",
    "        self.best_model = grid_search.best_estimator_\n",
    "        self.best_params = grid_search.best_params_\n",
    "\n",
    "        print(f\"Best parameters: {self.best_params}\")\n",
    "        print(f\"Best F1 score from Grid Search: {grid_search.best_score_}\")\n",
    "\n",
    "        # Perform cross-validation with the best model\n",
    "        kf = StratifiedKFold(n_splits=self.n_fold_cross_validation, shuffle=True, random_state=42)\n",
    "        f1_scores_macro = []\n",
    "\n",
    "        # Cross-validation loop to calculate the macro F1 score for each fold\n",
    "        for train_index, test_index in kf.split(self.train_data.drop('Target', axis=1), self.train_data['Target']):\n",
    "            X_train, X_test = self.train_data.drop('Target', axis=1).iloc[train_index], self.train_data.drop('Target', axis=1).iloc[test_index]\n",
    "            y_train, y_test = self.train_data['Target'].iloc[train_index], self.train_data['Target'].iloc[test_index]\n",
    "            \n",
    "            # Train the model on the training set\n",
    "            self.best_model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict on the test set\n",
    "            y_pred = self.best_model.predict(X_test)\n",
    "            \n",
    "            # Calculate the macro F1 score for this fold\n",
    "            f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "            f1_scores_macro.append(f1_macro)\n",
    "        \n",
    "        # Calculate the mean and standard deviation of the macro F1 scores\n",
    "        mean_f1_macro = np.mean(f1_scores_macro)\n",
    "        std_f1_macro = np.std(f1_scores_macro)\n",
    "\n",
    "        print(f\"\\n\\nCross-validation results:\")\n",
    "        print(f\"\\tMacro F1 Scores: {f1_scores_macro}\")\n",
    "        print(f\"\\tMean Macro F1 Score: {mean_f1_macro:.4f}\")\n",
    "        print(f\"\\tStandard Deviation of Macro F1 Score: {std_f1_macro:.4f}\")\n",
    "\n",
    "        return self.best_model, self.best_params\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        # Generate predictions using the best model\n",
    "        predictions = self.best_model.predict(test_data)\n",
    "        \n",
    "        # Define the reverse mapping from numeric to category\n",
    "        reverse_mapping = {0: 'low', 1: 'medium', 2: 'high'}\n",
    "        \n",
    "        # Map numeric predictions back to categories\n",
    "        return pd.Series(predictions).map(reverse_mapping)\n",
    "\n",
    "\n",
    "    def make_predictions(self, test_fname, predictions_fname):\n",
    "        # Save the columns to keep for consistency with the test set\n",
    "        self.columns_to_keep = list(self.train_data.columns)  # Save as a list to maintain order\n",
    "        print(\"Columns to keep:\", self.columns_to_keep)\n",
    "\n",
    "        test_data = pd.read_csv(test_fname)\n",
    "        self.originalTest = test_data\n",
    "        self.test_data = self.pre_process(data=test_data, dataType=\"test\")\n",
    "        predictions = self.predict(self.test_data)\n",
    "\n",
    "        # Step 4: Add UID column from `copy_test` DataFrame to the `reversed_predictions`\n",
    "        reversed_predictions_df = pd.DataFrame({\n",
    "            'UID': self.originalTest['UID'],\n",
    "            'Target': predictions  # The reversed prediction values\n",
    "        })\n",
    "        reversed_predictions_df.to_csv(predictions_fname, index=False)\n",
    "        print(\"File Made and prediction complete\")\n",
    "        return reversed_predictions_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost():\n",
    "    # Usage Example\n",
    "    # Assuming train_data and test_data are pandas DataFrames, and model is an estimator like XGBoost\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    # Example params list (you can adjust this for your model)\n",
    "    params_list = {\n",
    "        'learning_rate': [0.01],\n",
    "        'n_estimators': [100, 150, 200],\n",
    "        'max_depth': [7, 9]\n",
    "    }\n",
    "\n",
    "\n",
    "    # Assuming `train_data` and `test_data` are already defined\n",
    "    model = XGBClassifier(random_state=42)\n",
    "\n",
    "    # Create an instance of bestModel\n",
    "    best_model_instance = BestModel(model, \"train.csv\", params_list)\n",
    "\n",
    "    # Get the best model and its parameters\n",
    "    best_model, best_params = best_model_instance.best_model_outputer()\n",
    "\n",
    "    print(f\"Best Model of xGBoost with params : {best_params}\")\n",
    "    best_model_instance.make_predictions('test.csv','submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm():\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    # Example hyperparameter list for SVM with RBF kernel\n",
    "    params_list = {\n",
    "        'C': [0.1, 1, 10],  # Regularization parameter\n",
    "        'gamma': ['scale', 'auto', 0.1, 1],  # Kernel coefficient\n",
    "        'kernel': ['rbf']  # RBF kernel\n",
    "    }\n",
    "\n",
    "    # Create an SVM model instance\n",
    "    model = SVC(random_state=42)\n",
    "\n",
    "    # Create an instance of BestModel\n",
    "    best_model_instance = BestModel(model, \"train.csv\", params_list)\n",
    "\n",
    "    # Get the best model and its parameters\n",
    "    best_model, best_params = best_model_instance.best_model_outputer()\n",
    "\n",
    "    print(f\"Best Model of SVM with rbf kernal with params : {best_params}\")\n",
    "    best_model_instance.make_predictions('test.csv','submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest():\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    # Example hyperparameter list for Random Forest\n",
    "    params_list = {\n",
    "        'n_estimators': [100, 150, 200],  # Number of trees in the forest\n",
    "        'max_depth': [10, 20, 15],  # Maximum depth of the tree\n",
    "        'min_samples_split': [2, 5, 10]  # Minimum number of samples required to split a node\n",
    "    }\n",
    "\n",
    "    # Create a Random Forest model instance\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Create an instance of BestModel\n",
    "    best_model_instance = BestModel(model, \"train.csv\", params_list)\n",
    "\n",
    "    # Get the best model and its parameters\n",
    "    best_model, best_params = best_model_instance.best_model_outputer()\n",
    "\n",
    "    print(f\"Best Model of Random Forest with params: {best_params}\")\n",
    "    best_model_instance.make_predictions('test.csv', 'submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn():\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    # Example hyperparameter list for KNN\n",
    "    params_list = {\n",
    "        'n_neighbors': [3],  # Number of neighbors\n",
    "        'weights': ['distance'],  # Weight function used in prediction\n",
    "        'metric': ['manhattan']  # Distance metrics\n",
    "    }\n",
    "\n",
    "    # Create a KNN model instance\n",
    "    model = KNeighborsClassifier()\n",
    "\n",
    "    # Create an instance of BestModel\n",
    "    best_model_instance = BestModel(model, \"train.csv\", params_list)\n",
    "\n",
    "    # Get the best model and its parameters\n",
    "    best_model, best_params = best_model_instance.best_model_outputer()\n",
    "\n",
    "    print(f\"Best Model of KNN with params: {best_params}\")\n",
    "    best_model_instance.make_predictions('test.csv', 'submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_lstm():\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Load your training data\n",
    "    train_data = pd.read_csv(\"train.csv\")\n",
    "    X = train_data.iloc[:, :-1].values  # Assuming features are all columns except the last\n",
    "    y = train_data.iloc[:, -1].values   # Assuming the last column is the target\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Reshape the data for LSTM (samples, timesteps, features)\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_val = np.expand_dims(X_val, axis=2)\n",
    "\n",
    "    # Build a simple Bi-LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(Bidirectional(LSTM(32)))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Use 'softmax' for multi-class classification\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
    "\n",
    "    # Save predictions for the test data\n",
    "    test_data = pd.read_csv(\"test.csv\")\n",
    "    X_test = test_data.values\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Save predictions to submission.csv\n",
    "    pd.DataFrame(predictions, columns=[\"Predictions\"]).to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "    print(\"Bi-LSTM model training and predictions completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression():\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    # Example hyperparameter list for Linear Regression\n",
    "    params_list = {\n",
    "        'fit_intercept': [True, False],  # Whether to calculate the intercept for this model\n",
    "        'normalize': [True, False],  # Whether to normalize the data before fitting\n",
    "    }\n",
    "\n",
    "    # Create a Linear Regression model instance\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Create an instance of BestModel\n",
    "    best_model_instance = BestModel(model, \"train.csv\", params_list)\n",
    "\n",
    "    # Get the best model and its parameters\n",
    "    best_model, best_params = best_model_instance.best_model_outputer()\n",
    "\n",
    "    print(f\"Best Model of Linear Regression with params: {best_params}\")\n",
    "    best_model_instance.make_predictions('test.csv', 'submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    # Example hyperparameter list for Logistic Regression\n",
    "    params_list = {\n",
    "        'C': [0.1, 1, 10],  # Regularization strength\n",
    "        'penalty': ['l2'],  # Regularization type\n",
    "        'solver': ['lbfgs', 'liblinear'],  # Optimization algorithm\n",
    "        'max_iter': [100, 200]  # Maximum number of iterations for the solver\n",
    "    }\n",
    "\n",
    "    # Create a Logistic Regression model instance\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Create an instance of BestModel\n",
    "    best_model_instance = BestModel(model, \"train.csv\", params_list)\n",
    "\n",
    "    # Get the best model and its parameters\n",
    "    best_model, best_params = best_model_instance.best_model_outputer()\n",
    "\n",
    "    print(f\"Best Model of Logistic Regression with params: {best_params}\")\n",
    "    best_model_instance.make_predictions('test.csv', 'submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost_DT():\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    # Example params list for AdaBoost\n",
    "    params_list = {\n",
    "        'n_estimators': [50, 100, 150],  # Number of boosting rounds\n",
    "        'learning_rate': [0.01, 0.1, 1.0]  # Learning rate\n",
    "    }\n",
    "\n",
    "    # Initialize a weak classifier (e.g., DecisionTreeClassifier)\n",
    "    base_model = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "    # Create an AdaBoost model instance\n",
    "    model = AdaBoostClassifier(estimator=base_model, random_state=42)\n",
    "\n",
    "    # Create an instance of BestModel\n",
    "    best_model_instance = BestModel(model, \"train.csv\", params_list)\n",
    "\n",
    "    # Get the best model and its parameters\n",
    "    best_model, best_params = best_model_instance.best_model_outputer()\n",
    "\n",
    "    print(f\"Best Model of AdaBoost with params: {best_params}\")\n",
    "    best_model_instance.make_predictions('test.csv', 'submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "Best F1 score from Grid Search: 0.366029261808869\n",
      "\n",
      "\n",
      "Cross-validation results:\n",
      "\tMacro F1 Scores: [np.float64(0.36863244479178253), np.float64(0.3605061509492861), np.float64(0.3679886388341121), np.float64(0.35458606744591387), np.float64(0.36246185053319824), np.float64(0.36902247441358366), np.float64(0.3685488233494858), np.float64(0.37278861383531625), np.float64(0.3657064476481429), np.float64(0.373002794196934)]\n",
      "\tMean Macro F1 Score: 0.3663\n",
      "\tStandard Deviation of Macro F1 Score: 0.0054\n",
      "Best Model of KNN with params: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "Columns to keep: ['CropSpeciesVariety', 'CultivatedAreaSqft1', 'DistrictId', 'FarmEquipmentArea', 'FarmVehicleCount', 'FieldEstablishedYear', 'HarvestProcessingType', 'Latitude', 'Longitude', 'MainIrrigationSystemCount', 'NationalRegionCode', 'NaturalLakePresence', 'NumberOfFarmingZones', 'RawLocationId', 'StorageAndFacilityCount', 'TaxAgrarianValue', 'TaxLandValue', 'TotalCultivatedAreaSqft', 'TotalTaxAssessed', 'TotalValue', 'TownId', 'TypeOfIrrigationSystem', 'ValuationYear', 'WaterAccessPoints', 'WaterAccessPointsCalc', 'WaterReservoirCount', 'Target']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69744/3629377154.py:117: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[column] = data[column].fillna(self.missing_features[column])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Made and prediction complete\n"
     ]
    }
   ],
   "source": [
    "# xgboost()\n",
    "# svm()\n",
    "knn()\n",
    "# random_forest()\n",
    "# bi_lstm\n",
    "# logistic_regression()\n",
    "# linear_regression()\n",
    "# adaboost_DT()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['UID', 'AgriculturalPostalZone', 'AgricultureZoningCode',\n",
      "       'CropSpeciesVariety', 'CultivatedAreaSqft1', 'DistrictId',\n",
      "       'FarmEquipmentArea', 'FarmVehicleCount', 'FarmingCommunityId',\n",
      "       'FarmingUnitCount', 'FieldEstablishedYear', 'FieldSizeSqft',\n",
      "       'HarvestProcessingType', 'LandUsageType', 'Latitude', 'Longitude',\n",
      "       'MainIrrigationSystemCount', 'NationalRegionCode',\n",
      "       'NaturalLakePresence', 'NumberOfFarmingZones', 'OtherZoningCode',\n",
      "       'RawLocationId', 'SoilFertilityType', 'StorageAndFacilityCount',\n",
      "       'TaxAgrarianValue', 'TaxLandValue', 'TotalCultivatedAreaSqft',\n",
      "       'TotalTaxAssessed', 'TotalValue', 'TownId', 'TypeOfIrrigationSystem',\n",
      "       'ValuationYear', 'WaterAccessPoints', 'WaterAccessPointsCalc',\n",
      "       'WaterReservoirCount', 'Target'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69744/3629377154.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].fillna(mode_value)\n",
      "/tmp/ipykernel_69744/3629377154.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].fillna(mode_value)\n",
      "/tmp/ipykernel_69744/3629377154.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].fillna(mean_value)  # Fixing the warning by avoiding inplace=True\n",
      "/tmp/ipykernel_69744/3629377154.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].fillna(mode_value)  # Fixing the warning by avoiding inplace=True\n",
      "/tmp/ipykernel_69744/3629377154.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].fillna(mean_value)  # Fixing the warning by avoiding inplace=True\n",
      "/tmp/ipykernel_69744/3629377154.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].fillna(mode_value)  # Fixing the warning by avoiding inplace=True\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Example params list (you can adjust this for your model)\n",
    "params_list = {\n",
    "    'learning_rate': [0.01],\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [7, 9]\n",
    "}\n",
    "\n",
    "\n",
    "# Assuming `train_data` and `test_data` are already defined\n",
    "model = XGBClassifier(random_state=42)\n",
    "\n",
    "# Create an instance of bestModel\n",
    "best_model_instance = BestModel(model, \"train.csv\", params_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CropSpeciesVariety', 'CultivatedAreaSqft1', 'DistrictId',\n",
       "       'FarmEquipmentArea', 'FarmVehicleCount', 'FieldEstablishedYear',\n",
       "       'HarvestProcessingType', 'Latitude', 'Longitude',\n",
       "       'MainIrrigationSystemCount', 'NationalRegionCode',\n",
       "       'NaturalLakePresence', 'NumberOfFarmingZones', 'RawLocationId',\n",
       "       'StorageAndFacilityCount', 'TaxAgrarianValue', 'TaxLandValue',\n",
       "       'TotalCultivatedAreaSqft', 'TotalTaxAssessed', 'TotalValue', 'TownId',\n",
       "       'TypeOfIrrigationSystem', 'ValuationYear', 'WaterAccessPoints',\n",
       "       'WaterAccessPointsCalc', 'WaterReservoirCount', 'Target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_instance.train_data.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
