{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['UID', 'AgriculturalPostalZone', 'AgricultureZoningCode',\n",
      "       'CropFieldConfiguration', 'CropSpeciesVariety', 'CultivatedAndWildArea',\n",
      "       'CultivatedAreaSqft1', 'DistrictId', 'FarmClassification',\n",
      "       'FarmEquipmentArea', 'FarmShedAreaSqft', 'FarmVehicleCount',\n",
      "       'FarmingCommunityId', 'FarmingUnitCount', 'FieldConstructionType',\n",
      "       'FieldEstablishedYear', 'FieldShadeCover', 'FieldSizeSqft',\n",
      "       'FieldZoneLevel', 'HarvestProcessingType', 'HarvestStorageSqft',\n",
      "       'HasGreenHouse', 'HasPestControl', 'LandUsageType', 'Latitude',\n",
      "       'Longitude', 'MainIrrigationSystemCount', 'NationalRegionCode',\n",
      "       'NaturalLakePresence', 'NumberGreenHouses', 'NumberOfFarmingZones',\n",
      "       'OtherZoningCode', 'PartialIrrigationSystemCount',\n",
      "       'PerimeterGuardPlantsArea', 'PrimaryCropAreaSqft',\n",
      "       'PrimaryCropAreaSqft2', 'RawLocationId', 'ReservoirType',\n",
      "       'ReservoirWithFilter', 'SoilFertilityType', 'StorageAndFacilityCount',\n",
      "       'TaxAgrarianValue', 'TaxLandValue', 'TaxOverdueStatus',\n",
      "       'TaxOverdueYear', 'TotalAreaSqft', 'TotalCultivatedAreaSqft',\n",
      "       'TotalReservoirSize', 'TotalTaxAssessed', 'TotalValue', 'TownId',\n",
      "       'TypeOfIrrigationSystem', 'UndergroundStorageSqft', 'ValuationYear',\n",
      "       'WaterAccessPoints', 'WaterAccessPointsCalc', 'WaterReservoirCount',\n",
      "       'Target'],\n",
      "      dtype='object'), Length = 58\n"
     ]
    }
   ],
   "source": [
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "train_path = 'train.csv'\n",
    "test_path = 'test.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "train_data = pd.read_csv(train_path)\n",
    "test_data = pd.read_csv(test_path)\n",
    "\n",
    "print(f\"{train_data.columns}, Length = {len(train_data.columns)}\")\n",
    "data = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestModel:\n",
    "    def __init__(self, model=None, train_data_fname=\"'train.csv'\", params_list=None, n_fold_cross_validation=10):\n",
    "        self.model = model  # The model to be tuned\n",
    "        self.originalTrain = pd.read_csv(train_data_fname)\n",
    "        self.originalTest = None\n",
    "        self.train_data = self.pre_process(self.originalTrain, \"train\")  # Training data\n",
    "        # self.test_data = self.pre_process(test_data, \"test\")  # Testing data\n",
    "        self.test_data = None\n",
    "        self.params_list = params_list  # Hyperparameters to tune\n",
    "        self.best_model = None  # To store the best model after tuning\n",
    "        self.best_params = None  # To store the best parameters\n",
    "        self.n_fold_cross_validation = n_fold_cross_validation  # Number of folds for cross-validation\n",
    "        self.columns_to_keep = []  # Columns selected after pre-processing\n",
    "        self.feature_importance = {}  # Store feature importance (MI)\n",
    "\n",
    "    def map_categories_to_numeric(self, data, column_name=\"Target\"):\n",
    "        # Define the category mapping\n",
    "        category_mapping = {'low': 0, 'medium': 1, 'high': 2}\n",
    "        \n",
    "        # Map the column to numeric values\n",
    "        data[column_name] = data[column_name].map(category_mapping)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def replace_missing_values(self, data):\n",
    "        # Iterate over each column and replace missing values\n",
    "        for column in data.columns:\n",
    "            # Check if the column is of type float\n",
    "            if data[column].dtype == 'float64' or data[column].dtype == 'float32':\n",
    "                # Convert floats that represent integers to int\n",
    "                if (data[column] % 1 == 0).all():  # Check if all values in the column are effectively integers\n",
    "                    data[column] = data[column].astype('int64')  # Convert to integer type\n",
    "\n",
    "                # Replace missing values in float columns with the mean\n",
    "                mean_value = data[column].mean()\n",
    "                data[column] = data[column].fillna(mean_value)  # Fixing the warning by avoiding inplace=True\n",
    "\n",
    "            # Check if the column is of type int (or was converted from float to int)\n",
    "            elif data[column].dtype == 'int64' or data[column].dtype == 'int32':\n",
    "                # Replace missing values in integer columns with the mode\n",
    "                mode_value = data[column].mode()[0]\n",
    "                data[column] = data[column].fillna(mode_value)  # Fixing the warning by avoiding inplace=True\n",
    "\n",
    "        return data\n",
    "\n",
    "    def pre_process(self, data, dataType=\"train\"):\n",
    "        if dataType == \"train\":\n",
    "            data = self.map_categories_to_numeric(data)\n",
    "            \n",
    "            # Drop columns with more than a certain threshold of missing values\n",
    "            threshold = 0.8  # 80% missing values\n",
    "            threshold_count = int((1 - threshold) * len(data))\n",
    "            data = data.dropna(axis=1, thresh=threshold_count)\n",
    "\n",
    "            # Replace missing values in columns\n",
    "            data = self.replace_missing_values(data)\n",
    "\n",
    "            # Remove irrelevant columns based on feature-target relationship\n",
    "            data = self.remove_irrelevant_columns(data)\n",
    "\n",
    "\n",
    "        elif dataType == \"test\":\n",
    "            # Ensure all columns in self.columns_to_keep are in the test set\n",
    "            missing_columns = [col for col in self.columns_to_keep if col not in data.columns]\n",
    "            if missing_columns:\n",
    "                # Add missing columns to the test data with NaN values\n",
    "                for col in missing_columns:\n",
    "                    data[col] = pd.NA  # You can replace `pd.NA` with other placeholder values if needed\n",
    "                # print(f\"Added missing columns to the test data: {missing_columns}\")\n",
    "\n",
    "            # Drop any extra columns that are not in self.columns_to_keep\n",
    "            extra_columns = [col for col in data.columns if col not in self.columns_to_keep]\n",
    "            if extra_columns:\n",
    "                data = data.drop(columns=extra_columns)\n",
    "                # print(f\"Dropped extra columns from the test data: {extra_columns}\")\n",
    "\n",
    "\n",
    "            # Reindex columns in the same order as the training data\n",
    "            data = data.reindex(columns=self.columns_to_keep)\n",
    "\n",
    "            # Replace missing values in the test data\n",
    "            data = self.replace_missing_values(data)\n",
    "            data = data.drop(columns='Target')\n",
    "            \n",
    "            # print(f\"Final test data columns: {data.columns}\")\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def remove_irrelevant_columns(self, data):\n",
    "        # Calculate Mutual Information for classification (feature importance)\n",
    "        mutual_info = mutual_info_classif(data.drop('Target', axis=1), data['Target'])\n",
    "        feature_importance = dict(zip(data.drop('Target', axis=1).columns, mutual_info))\n",
    "        \n",
    "        # Threshold for mutual information to decide which columns are irrelevant\n",
    "        irrelevant_columns = [column for column, importance in feature_importance.items() if importance < 0.015]  # You can adjust this threshold\n",
    "        \n",
    "        print(f\"Removing columns with low mutual information: {irrelevant_columns}\")\n",
    "        data = data.drop(columns=irrelevant_columns)\n",
    "\n",
    "        # Pearson and Spearman correlations to remove correlated features\n",
    "        correlation_matrix = data.corr()\n",
    "        for column in data.columns:\n",
    "            try:\n",
    "                # Skip columns that are constant (no variance)\n",
    "                if data[column].nunique() == 1:\n",
    "                    # print(f\"Skipping {column} due to constant values.\")\n",
    "                    continue\n",
    "                \n",
    "                # Pearson Correlation with Target\n",
    "                pearson_corr, _ = pearsonr(data[column], data['Target'])\n",
    "                if abs(pearson_corr) < 0.015:  # A low correlation threshold\n",
    "                    # print(f\"Removing {column} due to low Pearson correlation with Target: {pearson_corr}\")\n",
    "                    data = data.drop(columns=[column])\n",
    "\n",
    "                # Spearman Correlation with Target\n",
    "                spearman_corr, _ = spearmanr(data[column], data['Target'])\n",
    "                if abs(spearman_corr) < 0.015:  # A low correlation threshold\n",
    "                    # print(f\"Removing {column} due to low Spearman correlation with Target: {spearman_corr}\")\n",
    "                    data = data.drop(columns=[column])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating correlation for {column}: {e}\")\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def best_model_outputer(self):\n",
    "        # Define the custom scoring function (F1 score)\n",
    "        f1_scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "        # Perform Grid Search with Stratified K-Fold and F1 score as the metric\n",
    "        grid_search = GridSearchCV(\n",
    "            self.model, \n",
    "            self.params_list, \n",
    "            cv=self.n_fold_cross_validation, \n",
    "            scoring=f1_scorer, \n",
    "            n_jobs=-2, \n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Use the entire training data for hyperparameter tuning\n",
    "        grid_search.fit(self.train_data.drop('Target', axis=1), self.train_data['Target'])\n",
    "        \n",
    "        # Store the best model and best parameters\n",
    "        self.best_model = grid_search.best_estimator_\n",
    "        self.best_params = grid_search.best_params_\n",
    "\n",
    "        print(f\"Best parameters: {self.best_params}\")\n",
    "        print(f\"Best F1 score from Grid Search: {grid_search.best_score_}\")\n",
    "\n",
    "        # Perform cross-validation with the best model\n",
    "        kf = StratifiedKFold(n_splits=self.n_fold_cross_validation, shuffle=True, random_state=42)\n",
    "        f1_scores_macro = []\n",
    "\n",
    "        # Cross-validation loop to calculate the macro F1 score for each fold\n",
    "        for train_index, test_index in kf.split(self.train_data.drop('Target', axis=1), self.train_data['Target']):\n",
    "            X_train, X_test = self.train_data.drop('Target', axis=1).iloc[train_index], self.train_data.drop('Target', axis=1).iloc[test_index]\n",
    "            y_train, y_test = self.train_data['Target'].iloc[train_index], self.train_data['Target'].iloc[test_index]\n",
    "            \n",
    "            # Train the model on the training set\n",
    "            self.best_model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict on the test set\n",
    "            y_pred = self.best_model.predict(X_test)\n",
    "            \n",
    "            # Calculate the macro F1 score for this fold\n",
    "            f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "            f1_scores_macro.append(f1_macro)\n",
    "        \n",
    "        # Calculate the mean and standard deviation of the macro F1 scores\n",
    "        mean_f1_macro = np.mean(f1_scores_macro)\n",
    "        std_f1_macro = np.std(f1_scores_macro)\n",
    "\n",
    "        print(f\"\\n\\nCross-validation results:\")\n",
    "        print(f\"\\tMacro F1 Scores: {f1_scores_macro}\")\n",
    "        print(f\"\\tMean Macro F1 Score: {mean_f1_macro:.4f}\")\n",
    "        print(f\"\\tStandard Deviation of Macro F1 Score: {std_f1_macro:.4f}\")\n",
    "\n",
    "        return self.best_model, self.best_params\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        # Generate predictions using the best model\n",
    "        predictions = self.best_model.predict(test_data)\n",
    "        \n",
    "        # Define the reverse mapping from numeric to category\n",
    "        reverse_mapping = {0: 'low', 1: 'medium', 2: 'high'}\n",
    "        \n",
    "        # Map numeric predictions back to categories\n",
    "        return pd.Series(predictions).map(reverse_mapping)\n",
    "\n",
    "\n",
    "    def make_predictions(self, test_fname, predictions_fname):\n",
    "        # Save the columns to keep for consistency with the test set\n",
    "        self.columns_to_keep = list(self.train_data.columns)  # Save as a list to maintain order\n",
    "        print(\"Columns to keep:\", self.columns_to_keep)\n",
    "\n",
    "        test_data = pd.read_csv(test_fname)\n",
    "        self.originalTest = test_data\n",
    "        self.test_data = self.pre_process(data=test_data, dataType=\"test\")\n",
    "        predictions = self.predict(self.test_data)\n",
    "\n",
    "        # Step 4: Add UID column from `copy_test` DataFrame to the `reversed_predictions`\n",
    "        reversed_predictions_df = pd.DataFrame({\n",
    "            'UID': self.originalTest['UID'],\n",
    "            'Target': predictions  # The reversed prediction values\n",
    "        })\n",
    "        reversed_predictions_df.to_csv(predictions_fname, index=False)\n",
    "        return reversed_predictions_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64554/52047531.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].fillna(mode_value)  # Fixing the warning by avoiding inplace=True\n",
      "/tmp/ipykernel_64554/52047531.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].fillna(mode_value)  # Fixing the warning by avoiding inplace=True\n",
      "/tmp/ipykernel_64554/52047531.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].fillna(mean_value)  # Fixing the warning by avoiding inplace=True\n",
      "/tmp/ipykernel_64554/52047531.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[column] = data[column].fillna(mode_value)  # Fixing the warning by avoiding inplace=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing columns with low mutual information: ['UID', 'AgriculturalPostalZone', 'AgricultureZoningCode', 'CropSpeciesVariety', 'DistrictId', 'FarmVehicleCount', 'FieldEstablishedYear', 'FieldSizeSqft', 'HarvestProcessingType', 'LandUsageType', 'MainIrrigationSystemCount', 'NationalRegionCode', 'NumberOfFarmingZones', 'OtherZoningCode', 'SoilFertilityType', 'StorageAndFacilityCount', 'TaxAgrarianValue', 'TaxLandValue', 'TotalCultivatedAreaSqft', 'TotalTaxAssessed', 'TotalValue', 'TypeOfIrrigationSystem', 'ValuationYear', 'WaterAccessPoints', 'WaterAccessPointsCalc', 'WaterReservoirCount']\n",
      "Removing FarmingCommunityId due to low Pearson correlation with Target: 0.0031323011796754218\n",
      "Error calculating correlation for FarmingCommunityId: 'FarmingCommunityId'\n",
      "Removing FarmingUnitCount due to low Pearson correlation with Target: -0.0015904990665650289\n",
      "Error calculating correlation for FarmingUnitCount: 'FarmingUnitCount'\n",
      "Removing Longitude due to low Pearson correlation with Target: 0.011229810056938037\n",
      "Error calculating correlation for Longitude: 'Longitude'\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "Best parameters: {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 2}\n",
      "Best F1 score from Grid Search: 0.25771311335184394\n",
      "\n",
      "\n",
      "Cross-validation results:\n",
      "\tMacro F1 Scores: [np.float64(0.260138724313643), np.float64(0.26176267547090226), np.float64(0.2574209949413475), np.float64(0.2530222792669157), np.float64(0.2581887196088459), np.float64(0.2624222605731322), np.float64(0.25761457860171055), np.float64(0.2552074122674039), np.float64(0.2596198690102162), np.float64(0.25745995287952517)]\n",
      "\tMean Macro F1 Score: 0.2583\n",
      "\tStandard Deviation of Macro F1 Score: 0.0027\n"
     ]
    }
   ],
   "source": [
    "# Usage Example\n",
    "# Assuming train_data and test_data are pandas DataFrames, and model is an estimator like XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Example params list (you can adjust this for your model)\n",
    "params_list = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "params_list = {\n",
    "    'learning_rate': [0.5],\n",
    "    'n_estimators': [2],\n",
    "    'max_depth': [3]\n",
    "}\n",
    "\n",
    "\n",
    "# Assuming `train_data` and `test_data` are already defined\n",
    "model = XGBClassifier(random_state=42)\n",
    "\n",
    "# Create an instance of bestModel\n",
    "best_model_instance = BestModel(model, \"train.csv\", params_list)\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model, best_params = best_model_instance.best_model_outputer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model of xGBoost with params : {'learning_rate': 0.5, 'max_depth': 3, 'n_estimators': 2}\n",
      "Columns to keep: ['CultivatedAreaSqft1', 'FarmEquipmentArea', 'Latitude', 'RawLocationId', 'TownId', 'Target']\n",
      "Added missing columns to the test data: ['Target']\n",
      "Dropped extra columns from the test data: ['UID', 'AgriculturalPostalZone', 'AgricultureZoningCode', 'CropFieldConfiguration', 'CropSpeciesVariety', 'CultivatedAndWildArea', 'DistrictId', 'FarmClassification', 'FarmShedAreaSqft', 'FarmVehicleCount', 'FarmingCommunityId', 'FarmingUnitCount', 'FieldConstructionType', 'FieldEstablishedYear', 'FieldShadeCover', 'FieldSizeSqft', 'FieldZoneLevel', 'HarvestProcessingType', 'HarvestStorageSqft', 'HasGreenHouse', 'HasPestControl', 'LandUsageType', 'Longitude', 'MainIrrigationSystemCount', 'NationalRegionCode', 'NaturalLakePresence', 'NumberGreenHouses', 'NumberOfFarmingZones', 'OtherZoningCode', 'PartialIrrigationSystemCount', 'PerimeterGuardPlantsArea', 'PrimaryCropAreaSqft', 'PrimaryCropAreaSqft2', 'ReservoirType', 'ReservoirWithFilter', 'SoilFertilityType', 'StorageAndFacilityCount', 'TaxAgrarianValue', 'TaxLandValue', 'TaxOverdueStatus', 'TaxOverdueYear', 'TotalAreaSqft', 'TotalCultivatedAreaSqft', 'TotalReservoirSize', 'TotalTaxAssessed', 'TotalValue', 'TypeOfIrrigationSystem', 'UndergroundStorageSqft', 'ValuationYear', 'WaterAccessPoints', 'WaterAccessPointsCalc', 'WaterReservoirCount']\n",
      "L S yaswanth kumar :  ['CultivatedAreaSqft1', 'FarmEquipmentArea', 'Latitude', 'RawLocationId', 'TownId', 'Target']\n",
      "Final test data columns: Index(['CultivatedAreaSqft1', 'FarmEquipmentArea', 'Latitude', 'RawLocationId',\n",
      "       'TownId'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>130000</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>129101</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>147876</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122624</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>159920</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15916</th>\n",
       "      <td>99588</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15917</th>\n",
       "      <td>86801</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15918</th>\n",
       "      <td>68439</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15919</th>\n",
       "      <td>13210</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15920</th>\n",
       "      <td>151497</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15921 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          UID  Target\n",
       "0      130000  medium\n",
       "1      129101  medium\n",
       "2      147876  medium\n",
       "3      122624  medium\n",
       "4      159920  medium\n",
       "...       ...     ...\n",
       "15916   99588  medium\n",
       "15917   86801  medium\n",
       "15918   68439  medium\n",
       "15919   13210  medium\n",
       "15920  151497  medium\n",
       "\n",
       "[15921 rows x 2 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Best Model of xGBoost with params : {best_params}\")\n",
    "best_model_instance.make_predictions('test.csv','submission.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
